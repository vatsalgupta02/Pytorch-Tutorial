{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3060, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10)\n",
    "                     )\n",
    "\n",
    "# Defining the Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Calculating the Logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculating the Loss\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLLLoss with log softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2991, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd- Working with Gradients\n",
    "\n",
    "Now that we know how to calculate a loss, how do we use it to perform backpropagation? Torch provides a module, autograd, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad=True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "---\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()`\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`\n",
    "\n",
    "The gradients are computed with respect to some variable z with `z.backward()`. This does a backward pass through the operations that created z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1798, -0.6411],\n",
      "        [ 0.2666, -1.4253]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3919, 0.4109],\n",
      "        [0.0710, 2.0314]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000257C5494E80>\n"
     ]
    }
   ],
   "source": [
    "# operation done for gradient\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9763, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# since we haven't still invoked the backwards method, the gradients are not yet calculated, therefore are empty\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While calculating mathematically, the gradient of *z* with respect to *x* is:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5899, -0.3205],\n",
      "        [ 0.1333, -0.7126]])\n",
      "tensor([[-0.5899, -0.3205],\n",
      "        [ 0.1333, -0.7126]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# computing the gradient\n",
    "z.backward()\n",
    "# this computes gradient of z w.r.t. x\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Loss and Autograd Together\n",
    "\n",
    "The gradients from the loss is used for updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2900, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     )\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 1.6325e-03,  1.6325e-03,  1.6325e-03,  ...,  1.6325e-03,\n",
      "          1.6325e-03,  1.6325e-03],\n",
      "        [-6.5324e-05, -6.5324e-05, -6.5324e-05,  ..., -6.5324e-05,\n",
      "         -6.5324e-05, -6.5324e-05],\n",
      "        [-2.2056e-04, -2.2056e-04, -2.2056e-04,  ..., -2.2056e-04,\n",
      "         -2.2056e-04, -2.2056e-04],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 2.4443e-04,  2.4443e-04,  2.4443e-04,  ...,  2.4443e-04,\n",
      "          2.4443e-04,  2.4443e-04],\n",
      "        [ 1.2610e-03,  1.2610e-03,  1.2610e-03,  ...,  1.2610e-03,\n",
      "          1.2610e-03,  1.2610e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general steps in  pytorch for training:\n",
    "\n",
    "- Make a forward pass through the network\n",
    "- Use the network output to calculate the loss\n",
    "- Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "- Take a step with the optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights: \n",
      " Parameter containing:\n",
      "tensor([[-0.0290, -0.0023,  0.0269,  ...,  0.0183,  0.0299, -0.0282],\n",
      "        [-0.0045, -0.0276, -0.0174,  ..., -0.0256,  0.0131,  0.0210],\n",
      "        [-0.0194,  0.0220, -0.0201,  ..., -0.0066, -0.0354,  0.0093],\n",
      "        ...,\n",
      "        [ 0.0089, -0.0083,  0.0199,  ..., -0.0333,  0.0296, -0.0076],\n",
      "        [-0.0023, -0.0268,  0.0169,  ..., -0.0058,  0.0144,  0.0014],\n",
      "        [-0.0285, -0.0091, -0.0101,  ..., -0.0348,  0.0015, -0.0008]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Gradient: \n",
      " tensor([[ 1.4998e-04,  1.4998e-04,  1.4998e-04,  ...,  1.4998e-04,\n",
      "          1.4998e-04,  1.4998e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.9934e-03,  1.9934e-03,  1.9934e-03,  ...,  1.9934e-03,\n",
      "          1.9934e-03,  1.9934e-03],\n",
      "        ...,\n",
      "        [-4.7124e-05, -4.7124e-05, -4.7124e-05,  ..., -4.7124e-05,\n",
      "         -4.7124e-05, -4.7124e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 3.1784e-03,  3.1784e-03,  3.1784e-03,  ...,  3.1784e-03,\n",
      "          3.1784e-03,  3.1784e-03]])\n"
     ]
    }
   ],
   "source": [
    "print(\"initial weights: \\n\", model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Pytorch by default accumulates the gradients by summing them up\n",
    "# therefore we need to clear all those gradients first\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "op = model(images)\n",
    "loss = criterion(op, labels)\n",
    "loss.backward()\n",
    "print('\\nGradient: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: \n",
      " Parameter containing:\n",
      "tensor([[-0.0290, -0.0023,  0.0269,  ...,  0.0183,  0.0299, -0.0282],\n",
      "        [-0.0045, -0.0276, -0.0174,  ..., -0.0256,  0.0131,  0.0210],\n",
      "        [-0.0194,  0.0220, -0.0201,  ..., -0.0066, -0.0354,  0.0093],\n",
      "        ...,\n",
      "        [ 0.0089, -0.0083,  0.0199,  ..., -0.0333,  0.0296, -0.0076],\n",
      "        [-0.0023, -0.0268,  0.0169,  ..., -0.0058,  0.0144,  0.0014],\n",
      "        [-0.0286, -0.0091, -0.0101,  ..., -0.0349,  0.0015, -0.0009]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Updating the weights\n",
    "optimizer.step()\n",
    "print('Updated weights: \\n', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
