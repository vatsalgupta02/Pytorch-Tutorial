{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting an image\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network and defining criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.698..  Test Loss: 1.012..  Test Accuracy: 0.628\n",
      "Epoch: 1/2..  Training Loss: 1.057..  Test Loss: 0.761..  Test Accuracy: 0.709\n",
      "Epoch: 1/2..  Training Loss: 0.898..  Test Loss: 0.689..  Test Accuracy: 0.735\n",
      "Epoch: 1/2..  Training Loss: 0.807..  Test Loss: 0.644..  Test Accuracy: 0.764\n",
      "Epoch: 1/2..  Training Loss: 0.758..  Test Loss: 0.610..  Test Accuracy: 0.769\n",
      "Epoch: 1/2..  Training Loss: 0.726..  Test Loss: 0.582..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.712..  Test Loss: 0.591..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.683..  Test Loss: 0.555..  Test Accuracy: 0.790\n",
      "Epoch: 1/2..  Training Loss: 0.643..  Test Loss: 0.574..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.614..  Test Loss: 0.528..  Test Accuracy: 0.802\n",
      "Epoch: 1/2..  Training Loss: 0.610..  Test Loss: 0.557..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.638..  Test Loss: 0.533..  Test Accuracy: 0.802\n",
      "Epoch: 1/2..  Training Loss: 0.619..  Test Loss: 0.524..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.628..  Test Loss: 0.528..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.513..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.621..  Test Loss: 0.498..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.579..  Test Loss: 0.523..  Test Accuracy: 0.805\n",
      "Epoch: 1/2..  Training Loss: 0.577..  Test Loss: 0.486..  Test Accuracy: 0.825\n",
      "Epoch: 1/2..  Training Loss: 0.582..  Test Loss: 0.486..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.572..  Test Loss: 0.501..  Test Accuracy: 0.818\n",
      "Epoch: 1/2..  Training Loss: 0.586..  Test Loss: 0.539..  Test Accuracy: 0.797\n",
      "Epoch: 1/2..  Training Loss: 0.565..  Test Loss: 0.475..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.473..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.535..  Test Loss: 0.465..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.547..  Test Loss: 0.474..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.573..  Test Loss: 0.467..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.534..  Test Loss: 0.460..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.465..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.552..  Test Loss: 0.476..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.466..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.550..  Test Loss: 0.467..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.522..  Test Loss: 0.459..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.554..  Test Loss: 0.472..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.468..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.456..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.463..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.459..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.502..  Test Loss: 0.467..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.544..  Test Loss: 0.456..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.468..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.529..  Test Loss: 0.456..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.525..  Test Loss: 0.448..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.494..  Test Loss: 0.433..  Test Accuracy: 0.842\n",
      "Epoch: 2/2..  Training Loss: 0.492..  Test Loss: 0.444..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.531..  Test Loss: 0.437..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.445..  Test Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading the Networks\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "\n",
      "The state dictionary keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our Model: \\n\\n\", model, '\\n\\n')\n",
    "print(\"The state dictionary keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the State_dict file\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "# Loading the state_dict file\n",
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-093b7260deaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This throws an error because the tensor sizes are wrong!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\miniconda3\\envs\\pyt\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1045\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This throws an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
